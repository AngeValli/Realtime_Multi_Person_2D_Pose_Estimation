{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Realtime_Multi_Person_2D_Pose_Estimation_Ange_Valli.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT7U8uUN5f9B"
      },
      "source": [
        "See original notebook : https://colab.research.google.com/drive/1vm6n4ZvOf7RpZhGvA6p5JE0QcRYYkJXx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfUgH008CctZ"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Che0-uRWCe1w"
      },
      "source": [
        "Download dataset and required files from https://drive.google.com/drive/folders/1AXV-CXs4D4fn8ub8oBV4G6mB6viqBJbm?usp=sharing before running. Put the `RT-multiperson-pose-pytorch` folder in the same folder of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bpl2i5PKYwH"
      },
      "source": [
        "#Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields\n",
        "\n",
        "This notebook is exploring the algorithm for human pose estimation OpenPose. The original repository is in https://github.com/CMU-Perceptual-Computing-Lab/openpose and the pytorch implementation used here comes from  https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation.\n",
        "\n",
        "This notebook is based on the original notebook by Sao Mai Nguyen.\n",
        "\n",
        "Please make sure that the mounted Google Drive contains the needed ressources and data for the project. All the cells of the \"Settings\" part need to be executed in order to run the code of the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUcPLCl2mTts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24dfc137-42ad-4e09-dd24-ecaf560e46b5"
      },
      "source": [
        "# Libraries\n",
        "# Colab libraries\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "drive.mount('/content/gdrive')\n",
        "colab_path = \"/content/gdrive/My Drive/Colab Notebooks/Projects_for_Github\" # CHANGE PATH TO FIND\n",
        "\n",
        "# Basis libraries\n",
        "import os, re, sys, math, time, scipy, argparse\n",
        "import cv2, matplotlib\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from collections import OrderedDict\n",
        "from scipy.ndimage.morphology import generate_binary_structure\n",
        "from scipy.ndimage.filters import gaussian_filter, maximum_filter"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H4fYSU2rIYe"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faXg6vkalZ_L"
      },
      "source": [
        "First, we install the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NvO-_0elReB"
      },
      "source": [
        "# Independence install\n",
        "!sudo apt-get install swig\n",
        "\n",
        "\n",
        "# Repository compile\n",
        "%cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
        "%cd lib/pafprocess \n",
        "!sh make.sh\n",
        "\n",
        "# Libraries install\n",
        "%cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
        "!python -m pip install -r ./requirements.txt\n",
        "!pip3 install numpngw\n",
        "\n",
        "print(\"[INFO]: Project requirements installed successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViCpmMNFmorG"
      },
      "source": [
        "Now, we can import others libraries use in the project, contained in the repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6nPkoWXWgQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a54a28-90fb-49d8-be83-9ec370a54967"
      },
      "source": [
        "# Framework libraries\n",
        "%cd $colab_path\"/RT-multiperson-pose-pytorch\"\n",
        "sys.path.append('.'); sys.argv=['']\n",
        "from lib.network.rtpose_vgg import get_model \n",
        "from lib.network import im_transform\n",
        "from evaluate.coco_eval import get_outputs, handle_paf_and_heat\n",
        "from lib.utils.common import Human, BodyPart, CocoPart, CocoColors, CocoPairsRender, draw_humans\n",
        "from lib.utils.paf_to_pose import paf_to_pose_cpp\n",
        "from lib.config import cfg, update_config\n",
        "from torchsummary import summary\n",
        "from evaluate.coco_eval import get_outputs, handle_paf_and_heat, run_eval"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/Projects_for_Github/RT-multiperson-pose-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beVjL_eYnyjk"
      },
      "source": [
        "And update the variables space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfAiXpIpXITj"
      },
      "source": [
        "###########NETWORK CONFIG ############\n",
        "class Namespace:\n",
        "  def __init__(self, **kwargs):\n",
        "    self.__dict__.update(kwargs)\n",
        "\n",
        "# update config file\n",
        "args = Namespace(cfg = './experiments/vgg19_368x368_sgd.yaml', weight = 'pose_model.pth', opts = [])\n",
        "update_config(cfg, args)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8keHB6yvKgVO"
      },
      "source": [
        "# Other import\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import Image\n",
        "from numpngw import write_apng"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocOYORbPMKBM"
      },
      "source": [
        "### ***Model extraction***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHPvvYN7_vLE"
      },
      "source": [
        "Initially, the input dimension image () is introduced into the first 10 layers of the CNN VGG-19 model, which is normally used for image classification, and produces a set of feature maps F that is input to the first stage of each branch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RldOVfhWLdpJ"
      },
      "source": [
        "In the following code fragment you can see the detail of the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYG1V-IfXNaB"
      },
      "source": [
        "model = get_model('vgg19')   \n",
        "model.load_state_dict(torch.load(args.weight))\n",
        "model = torch.nn.DataParallel(model).cuda()\n",
        "model.float()\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghoiLdkis0LX"
      },
      "source": [
        "# Part 1 : Blur Face\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1epqnDN7eeK"
      },
      "source": [
        "### Detect-and-blur function\n",
        "\n",
        "In the following cells, the function `blur_faces` detects and blurs the faces in a unique given frame. It uses default model and configuration as default arguments.\n",
        "\n",
        "To blur the faces, we seek for each detected human the position and radius of a circle which contains his or her head.\n",
        "- The position of the center is given by the trained model, with keypoint number 0 which directly corresponds to the estimated position of the head / the nose.\n",
        "- The radius of the circle is based on distances between different keypoints. We take the maximum of :\n",
        "  - the length of the neck (between keypoints 0 and 1)\n",
        "  - the distance between the ears (keypoints 16 and 17)\n",
        "  - 75% of the distance between both shoulder (keypoints 2 and 5)\n",
        "\n",
        "This apparently complicated formula simply ensures that the value of the radius is big enough to anonimize the people *no matter the point of view*. For example, the length of the neck of person seen from above could be considered as null. On the contrary, a side view may align ears and shoulders (the keypoints are very close to each other), but let the neck visible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJUUqInqGB2_"
      },
      "source": [
        "# Distance functions between different\n",
        "# parts of the body\n",
        "\n",
        "def dist_neck(human):\n",
        "  \"\"\"Return the length of the neck\"\"\"\n",
        "  if 0 not in human.body_parts.keys():\n",
        "    return 0\n",
        "  if 1 not in human.body_parts.keys():\n",
        "    return 0\n",
        "  head = human.body_parts[0] # 0 for the head/nose\n",
        "  m_shoulder = human.body_parts[1] # 1 for the neck\n",
        "  length = int(np.sqrt(\n",
        "    ((m_shoulder.x-head.x)*frame_width)**2\n",
        "    + ((m_shoulder.y-head.y)*frame_height)**2\n",
        "  ))\n",
        "  return length\n",
        "\n",
        "def dist_ears(human):\n",
        "  \"\"\"Return the distance between the two ears\"\"\"\n",
        "  if 16 not in human.body_parts.keys():\n",
        "    return 0\n",
        "  if 17 not in human.body_parts.keys():\n",
        "    return 0\n",
        "  r_ear = human.body_parts[16] # 2 for the right ear\n",
        "  l_ear = human.body_parts[17] # 5 for the left ear\n",
        "  length = int(np.sqrt(\n",
        "    ((l_ear.x-r_ear.x)*frame_width)**2\n",
        "    + ((l_ear.y-r_ear.y)*frame_height)**2\n",
        "  ))\n",
        "  return length\n",
        "\n",
        "def dist_shoulders(human):\n",
        "  \"\"\"Return the distance between the shoulders\"\"\"\n",
        "  if 2 not in human.body_parts.keys():\n",
        "    return 0\n",
        "  if 5 not in human.body_parts.keys():\n",
        "    return 0\n",
        "  r_shoulder = human.body_parts[2] # 2 for the right shoulder\n",
        "  l_shoulder = human.body_parts[5] # 5 for the left shoulder\n",
        "  length = np.sqrt(\n",
        "    ((l_shoulder.x-r_shoulder.x)*frame_width)**2\n",
        "    + ((l_shoulder.y-r_shoulder.y)*frame_height)**2\n",
        "  )\n",
        "  return length"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vXGaBE46M6C"
      },
      "source": [
        "# Face-blurring function for a single frame\n",
        "\n",
        "def blur_faces(oriImg,\n",
        "      frame_width, frame_height,\n",
        "      model=model, cfg=cfg):\n",
        "    # Detect points of interest\n",
        "    with torch.no_grad():\n",
        "      paf, heatmap, imscale = get_outputs(\n",
        "        oriImg, model, 'rtpose'\n",
        "      )\n",
        "    humans = paf_to_pose_cpp(heatmap, paf, cfg)\n",
        "\n",
        "    # Iterate on each detected human to build the mask\n",
        "    mask = np.zeros((frame_height, frame_width), dtype=np.uint8)\n",
        "    for human in humans:\n",
        "      # Center : Find the head\n",
        "      if 0 not in human.body_parts.keys():\n",
        "        continue\n",
        "      head = human.body_parts[0] # 0 is the index for the head/nose\n",
        "      center = (\n",
        "        int(head.x * frame_width + 0.5),\n",
        "        int(head.y * frame_height + 0.5)\n",
        "      )\n",
        "      # Radius : maximum of different lengths of the body\n",
        "      ears_radius = dist_ears(human)\n",
        "      neck_radius = dist_neck(human)\n",
        "      shoulder_radius = 0.75 * dist_shoulders(human)\n",
        "      radius = int(max(ears_radius, neck_radius, shoulder_radius))\n",
        "\n",
        "      # Add the blurred area to the mask\n",
        "      cv2.circle(mask, center, radius, (255, 255, 255), -1)\n",
        "\n",
        "    # Compute a blurred version of the frame\n",
        "    blurredArray = cv2.GaussianBlur(oriImg, (15, 15), 11)\n",
        "    # Build the blurred-faces frame\n",
        "    mask = mask / 255.0\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    image = (1.0 - mask) * oriImg + mask * blurredArray\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    return image"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuh5_aOJ9UB9"
      },
      "source": [
        "### Run the experiments\n",
        "\n",
        "First of all, select a video to be blurred. If you choose to activate the test mode, only the ten first frames of the video will be considered.\n",
        "\n",
        "Then, you can run the next cell to launch the operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MGKtFsU-LhK",
        "cellView": "form"
      },
      "source": [
        "#@markdown Parameters :\n",
        "filename = \"./dataTpPoseKeraal/ctk/data1/Vid003.mp4\" #@param [\"./dataTpPoseKeraal/ctk/data1/Vid003.mp4\", \"./dataTpPoseKeraal/ctk/data2/Vid0015.mp4\", \"./dataTpPoseKeraal/ctk/data3/VideoColorCorrect0.mp4\", \"./dataTpPoseKeraal/rtk/data4/Vid008.mp4\", \"./dataTpPoseKeraal/rtk/data5/Vid021.mp4\", \"./dataTpPoseKeraal/rtk/data6/VideoColorCorrect0.mp4\"]\n",
        "TEST_MODE = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APBN57gs_jNG"
      },
      "source": [
        "# Main cell\n",
        "## Initialization\n",
        "\n",
        "images =[]\n",
        "video_capture = cv2.VideoCapture(filename)\n",
        "\n",
        "# Set resolutions for the output file\n",
        "frame_width = int(video_capture.get(3))\n",
        "frame_height = int(video_capture.get(4))\n",
        "size = (frame_width, frame_height)\n",
        "\n",
        "# Prepare output\n",
        "result = cv2.VideoWriter(\n",
        "  './filename.avi',  \n",
        "  cv2.VideoWriter_fourcc(*'MJPG'), \n",
        "  10,\n",
        "  size\n",
        ")\n",
        "\n",
        "## Main loop : loop over the frames\n",
        "\n",
        "i = 1 # frame index\n",
        "while video_capture.isOpened():\n",
        "  # Capture frame-by-frame\n",
        "  is_success, oriImg = video_capture.read()\n",
        "  if not is_success:\n",
        "    break\n",
        "\n",
        "  # Generate final frame, with blurred faces\n",
        "  image = blur_faces(oriImg, frame_width, frame_height)\n",
        "\n",
        "  # Save the frame for output\n",
        "  output.clear()\n",
        "  images.append(image)\n",
        "  result.write(image)\n",
        "\n",
        "  # Early stopping if in test mode :\n",
        "  if TEST_MODE and i >= 10:\n",
        "    break\n",
        "  i += 1\n",
        "\n",
        "# Save output\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t556l_tHQsaH"
      },
      "source": [
        "# Display the result\n",
        "write_apng('outvideo.png', images, delay=20)\n",
        "Image(filename='outvideo.png')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFNtgDGDPlY2"
      },
      "source": [
        "The algorithm shows globally satisfying results. We are no longer able to identify the faces of the people in the videos.\n",
        "\n",
        "Some issues may still occur in the first video, for example. When the third person hides his face, the skeleton recognition is disturbed and it takes time to blur again, at the end. A trick to implement could be to compare each head found with the heads of previous frame. After a meticulous sort, we can identity missing ones and blur the same areas from previous analysis, with the hypothesis that it does not move a lot.\n",
        "\n",
        "Furthermore, a better rendering could by obtained by smoothing the position and radius of blurring circles between successive frames. Nevertheless, the current method used to compute the radius is based on the skeleton size : it adapts rather efficiently to sudden changes and is not too noisy if the movements of the people are smooth enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXqyuPt29nKb"
      },
      "source": [
        "# Part 2 Classify the movements\n",
        "\n",
        "There are 3 videos. We use 2 of them to train a model, and the 3rd video for testing.\n",
        "\n",
        "We refer to the pytorch tutorial on sequence models in https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html and the documentation on torch.nn on https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l30VKES-lHE"
      },
      "source": [
        "#Project2 : Use one of the representations of movements to model the two exercises and classify them.\n",
        "\n",
        "# Importing modules\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, LSTM\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "if colab_path[-1] != \"/\" :\n",
        "  colab_path = colab_path + \"/\"\n",
        "\n",
        "# Importing videos\n",
        "videos_link = [\n",
        "  'RT-multiperson-pose-pytorch/dataTpPoseKeraal/ctk/data1/Vid003.mp4',\n",
        "  'RT-multiperson-pose-pytorch/dataTpPoseKeraal/ctk/data2/Vid015.mp4', \n",
        "  'RT-multiperson-pose-pytorch/dataTpPoseKeraal/ctk/data3/VideoColor_Correct0.avi', \n",
        "  'RT-multiperson-pose-pytorch/dataTpPoseKeraal/rtk/data4/Vid008.mp4', \n",
        "  'RT-multiperson-pose-pytorch/dataTpPoseKeraal/rtk/data5/Vid021.mp4', \n",
        "  'RT-multiperson-pose-pytorch/dataTpPoseKeraal/rtk/data6/VideoColor_Correct0.avi'\n",
        "  ]\n",
        "ctk_videos = []\n",
        "rtk_videos = []\n",
        "\n",
        "for video_link in videos_link :\n",
        "  if \"ctk\" in video_link :\n",
        "    ctk_videos.append(colab_path + video_link)\n",
        "  elif \"rtk\" in video_link :\n",
        "    rtk_videos.append(colab_path + video_link)\n",
        "  else :\n",
        "    print(\"Erreur sur les liens\")\n",
        "    break\n",
        "\n",
        "# Building data sets and labels sets\n",
        "data = []\n",
        "labels = []\n",
        "test_ctk = []\n",
        "test_rtk = []\n",
        "\n",
        "# CTK for training\n",
        "for link in ctk_videos[0:2] :\n",
        "  video = cv2.VideoCapture(link)\n",
        "  frame_width = int(video.get(3)) \n",
        "  frame_height = int(video.get(4)) \n",
        "  size = (frame_width, frame_height)\n",
        "  while video.isOpened():\n",
        "    label = 'ctk'\n",
        "    success, image_raw = video.read()\n",
        "    if not success :\n",
        "      break\n",
        "    image = cv2.resize(cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB), size)\n",
        "    data.append(image)\n",
        "    labels.append(label)\n",
        "\n",
        "# CTK for testing\n",
        "link = ctk_videos[2]\n",
        "video = cv2.VideoCapture(link)\n",
        "while video.isOpened():\n",
        "  success, image_raw = video.read()\n",
        "  if not success :\n",
        "    break\n",
        "  image = cv2.resize(cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB), size)\n",
        "  test_ctk.append(image)\n",
        "test_ctk = np.array(test_ctk)\n",
        "\n",
        "# RTK for training\n",
        "for link in rtk_videos[0:2] :\n",
        "  video = cv2.VideoCapture(link)\n",
        "  frame_width = int(video.get(3)) \n",
        "  frame_height = int(video.get(4)) \n",
        "  size = (frame_width, frame_height)\n",
        "  while video.isOpened():\n",
        "    label = 'rtk'\n",
        "    success, image_raw = video.read()\n",
        "    if not success :\n",
        "      break      \n",
        "    image = cv2.resize(cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB), size)\n",
        "    data.append(image)\n",
        "    labels.append(label)\n",
        "\n",
        "# RTK for testing\n",
        "link = rtk_videos[2]\n",
        "video = cv2.VideoCapture(link)\n",
        "while video.isOpened():\n",
        "  success, image_raw = video.read()\n",
        "  if not success :\n",
        "    break      \n",
        "  image = cv2.resize(cv2.cvtColor(image_raw, cv2.COLOR_BGR2RGB), size)\n",
        "  test_rtk.append(image)\n",
        "test_rtk = np.array(test_rtk)\n",
        "\n",
        "# Building training and validation set for training\n",
        "data = np.array(data)\n",
        "# One-Hot encoding of labels\n",
        "OneHotLabels = np.array([\n",
        "     [1,0] if label == 'ctk' else [0,1] for label in np.array(labels)\n",
        "     ])\n",
        "# Splits of training and testing sets\n",
        "(trainX, testX, trainY, testY) = train_test_split(\n",
        "                                  data, \n",
        "                                  OneHotLabels, \n",
        "                                  test_size=0.2, \n",
        "                                  stratify=OneHotLabels, \n",
        "                                  random_state=42\n",
        "                                 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIW-90tUXTYV"
      },
      "source": [
        "We build the dataset so each sample corresponds to a frame.\n",
        "The proportion of split for training and validation set is 80%/20%.\n",
        "The _stratify_ option is activated so the split is done in a way that preserves the same proportions of examples in each class, here we pass the one-hot encoding of the labels as parameter so the proportion of frames representing one movement is 50% of training and testing datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcQJG3Z2bITZ"
      },
      "source": [
        "The model is implemented using Keras and it is inspired from the neural network refered in :\n",
        "\n",
        "_Patrice Ferlet_ \"Training a neural network with an image sequence â€” example with a video as input\" _Smile Innovation, Nov 2019_ [link](https://medium.com/smileinnovation/training-neural-network-with-image-sequence-an-example-with-video-as-input-c3407f7a0b0f)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7yND7KTcaym"
      },
      "source": [
        "# Model\n",
        "InputModel = ResNet50(weights=\"imagenet\", \n",
        "                      include_top=False, \n",
        "                      input_tensor=Input(shape=(frame_height, frame_width, 3))\n",
        "             )\n",
        "\n",
        "input_layer = InputModel.output\n",
        "averagepool_layer = AveragePooling2D(pool_size=(7, 7))(input_layer)\n",
        "flatten_layer = Flatten(name=\"flatten\")(averagepool_layer)\n",
        "dense_layer = Dense(512, activation=\"relu\")(flatten_layer)\n",
        "dropout_layer = Dropout(0.5)(dense_layer)\n",
        "output_layer = Dense(2, activation=\"softmax\")(dropout_layer)\n",
        "\n",
        "model = Model(inputs=InputModel.input, outputs=output_layer)\n",
        "\n",
        "for layer in InputModel.layers :\n",
        "  layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p82F-moZdYu1"
      },
      "source": [
        "# Parameters \n",
        "nb_epochs = 3\n",
        "learning_rate = 1e-5\n",
        "momentum = 0.9\n",
        "weight_decay = 1e-5 / nb_epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGJiusu9dbfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddf4abc-7f65-4687-eec7-40506d783026"
      },
      "source": [
        "# Defining optimizer and compiling model\n",
        "optim = SGD(lr=learning_rate, momentum=momentum, decay=weight_decay)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=optim, metrics=[\"accuracy\"])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGfPb-SZdiTE"
      },
      "source": [
        "# Fitting model on data and plotting training metrics\n",
        "fitted_model = model.fit(x=trainX,\n",
        "                         y=trainY,\n",
        "                         steps_per_epoch=len(trainX) // 64, \n",
        "                         validation_data=(testX, testY), \n",
        "                         epochs=nb_epochs\n",
        "               )\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, nb_epochs),\n",
        "         fitted_model.history[\"loss\"], \n",
        "         label=\"train_loss\")\n",
        "plt.plot(np.arange(0, nb_epochs), \n",
        "         fitted_model.history[\"val_loss\"], \n",
        "         label=\"validation_loss\")\n",
        "plt.plot(np.arange(0, nb_epochs), \n",
        "         fitted_model.history[\"accuracy\"], \n",
        "         label=\"train_accuracy\")\n",
        "plt.title(\"Loss and Accuracy\")\n",
        "plt.xlabel(\"Nb_epochs\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DvfpiA_dvJE"
      },
      "source": [
        "We use the model we trained on the two first videos to classify the third video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnqLO_Pqd1-7"
      },
      "source": [
        "# Testing\n",
        "predictions_ctk = model.predict(x=test_ctk.astype(\"float32\"), batch_size=32)\n",
        "predictions_rtk = model.predict(x=test_rtk.astype(\"float32\"), batch_size=32)\n",
        "\n",
        "if np.argmax(predictions_ctk.mean(axis=0)) :\n",
        "  result_ctk = \"ctk\"\n",
        "else :\n",
        "  result_ctk = \"rtk\"\n",
        "print(\"CTK video will be labelled\", result_ctk)\n",
        "print(\"Accuracy :\", predictions_ctk.mean(axis=0)[1] * 100, \"%\")\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "if np.argmax(predictions_rtk.mean(axis=0)) :\n",
        "  result_rtk = \"ctk\"\n",
        "else :\n",
        "  result_rtk = \"rtk\"\n",
        "print(\"RTK video will be labelled\", result_rtk)\n",
        "print(\"Accuracy :\", predictions_rtk.mean(axis=0)[0] * 100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIe04NTPkhsH"
      },
      "source": [
        "The model should not be trained over a too high number of epochs as the network tends to overfit quickly, due to the fact that we don't have a large dataset of inputs. The negative effect is the weakness of the average predictions for each video, as the accuracies never exceed 60%.\n",
        "\n",
        "The training is unstable and the stochastic aspect of it induces errors. It may not always build a model leading to the expected predictions. After several runs, the model seems to overfit even if we are building a new model each time. We observe that the accuracy at the end of the first epoch is higher, and then it is necessary to kill the kernel to perform a training from scratch."
      ]
    }
  ]
}